{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d348ff41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Training Window: Up to 2011-09-10\n",
      "Target Window:   After 2011-09-10\n",
      "\n",
      "==================================================\n",
      "CLUSTERING METRICS\n",
      "==================================================\n",
      "Metric                    | Value     \n",
      "----------------------------------------\n",
      "Silhouette Coeff          | 0.3539\n",
      "Davies-Bouldin Index      | 1.0516\n",
      "Calinski-Harabasz         | 2727.1\n",
      "\n",
      "==================================================\n",
      "SEGMENT PROFILES\n",
      "==================================================\n",
      "               Count  Share (%)  Recency  Frequency  Avg_Monthly_Spend  FutureSales_Label\n",
      "Segment_Label                                                                            \n",
      "At-Risk         1375      40.80   168.52       1.34              51.52             310.12\n",
      "Average         1438      42.67    55.73       2.72             251.03             505.80\n",
      "High Value       557      16.53    17.83      11.05             765.54            3048.69\n",
      "\n",
      "Saved processed data to: C:\\Users\\arnav\\OneDrive\\Desktop\\customer-risk-agent\\data\\processed\\labeled_customers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIGURATION & LOAD\n",
    "print(\"Loading dataset...\")\n",
    "FILE_PATH = r'C:\\Users\\arnav\\OneDrive\\Desktop\\customer-risk-agent\\data\\raw\\Online_Retail .xlsx'\n",
    "PROCESSED_PATH = r'C:\\Users\\arnav\\OneDrive\\Desktop\\customer-risk-agent\\data\\processed\\labeled_customers.csv'\n",
    "\n",
    "# Handle relative paths for flexibility\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    FILE_PATH = os.path.join(os.getcwd(), '..', 'data', 'raw', 'Online_Retail.xlsx')\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_excel(FILE_PATH)\n",
    "except:\n",
    "    df_raw = pd.read_csv(FILE_PATH, encoding='ISO-8859-1')\n",
    "\n",
    "df_raw['InvoiceDate'] = pd.to_datetime(df_raw['InvoiceDate'])\n",
    "\n",
    "# Pre-calculate returns to avoid data loss during filtering\n",
    "returns_df = df_raw[df_raw['Quantity'] < 0].groupby('CustomerID')['Quantity'].count().rename('Return_Count')\n",
    "\n",
    "# Filter for valid sales\n",
    "df = df_raw[(df_raw['Quantity'] > 0) & (df_raw['UnitPrice'] > 0)].dropna(subset=['CustomerID'])\n",
    "\n",
    "# SPLIT STRATEGY (90-Day Lookahead)\n",
    "max_date = df['InvoiceDate'].max()\n",
    "cutoff_date = max_date - pd.Timedelta(days=90)\n",
    "\n",
    "df_past = df[df['InvoiceDate'] <= cutoff_date].copy()\n",
    "df_future = df[df['InvoiceDate'] > cutoff_date].copy()\n",
    "\n",
    "print(f\"Training Window: Up to {cutoff_date.date()}\")\n",
    "print(f\"Target Window:   After {cutoff_date.date()}\")\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "snapshot_date = cutoff_date + pd.Timedelta(days=1)\n",
    "\n",
    "# Aggregation: Recency, Frequency, Tenure, Monetary\n",
    "rfm = df_past.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': [\n",
    "        lambda x: (snapshot_date - x.max()).days, # Recency\n",
    "        lambda x: (snapshot_date - x.min()).days  # Tenure\n",
    "    ],\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'Quantity': 'sum',\n",
    "    'UnitPrice': lambda x: (x * df_past.loc[x.index, 'Quantity']).sum()\n",
    "}).fillna(0)\n",
    "\n",
    "rfm.columns = ['Recency', 'Tenure', 'Frequency', 'Total_Items', 'Monetary_Sum']\n",
    "\n",
    "# Merge metadata\n",
    "rfm = rfm.merge(returns_df, on='CustomerID', how='left')\n",
    "rfm['Return_Count'] = rfm['Return_Count'].fillna(0)\n",
    "\n",
    "# Velocity Metrics\n",
    "rfm['Tenure_Capped'] = rfm['Tenure'].clip(lower=30) \n",
    "recent_start = cutoff_date - pd.Timedelta(days=90)\n",
    "recent_stats = df_past[df_past['InvoiceDate'] > recent_start].groupby('CustomerID')['InvoiceNo'].nunique().rename('Freq_Recent')\n",
    "rfm = rfm.merge(recent_stats, on='CustomerID', how='left').fillna(0)\n",
    "\n",
    "rfm['Velocity_Recent'] = rfm['Freq_Recent'] / 3\n",
    "rfm['Velocity_Life'] = rfm['Frequency'] / (rfm['Tenure_Capped'] / 30)\n",
    "rfm['Velocity_Drift'] = rfm['Velocity_Recent'] - rfm['Velocity_Life']\n",
    "\n",
    "# Normalized Spend (Avg Monthly)\n",
    "rfm['Avg_Monthly_Spend'] = rfm['Monetary_Sum'] / (rfm['Tenure_Capped'] / 30)\n",
    "\n",
    "# Cap AOV outliers at 99th percentile\n",
    "rfm['AOV'] = rfm['Monetary_Sum'] / rfm['Frequency']\n",
    "cap_aov = rfm['AOV'].quantile(0.99)\n",
    "rfm['AOV'] = rfm['AOV'].clip(upper=cap_aov)\n",
    "\n",
    "# Feature subset for model\n",
    "features = ['Recency', 'Frequency', 'Avg_Monthly_Spend', 'Tenure', 'AOV', 'Velocity_Recent', 'Velocity_Drift', 'Return_Count']\n",
    "rfm_clean = rfm[features].copy()\n",
    "\n",
    "\n",
    "# CLUSTERING (K-Means)\n",
    "\n",
    "\n",
    "# Log transform for skew handling\n",
    "rfm_log = np.log1p(rfm_clean[['Recency', 'Frequency', 'Avg_Monthly_Spend']])\n",
    "scaler = StandardScaler()\n",
    "rfm_scaled = scaler.fit_transform(rfm_log)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "rfm_clean['Cluster'] = kmeans.fit_predict(rfm_scaled)\n",
    "\n",
    "# Dynamic Labeling (Centroid Analysis)\n",
    "summary = rfm_clean.groupby('Cluster').agg({'Recency': 'mean', 'Avg_Monthly_Spend': 'mean'}).reset_index()\n",
    "vip_idx = summary.sort_values('Avg_Monthly_Spend', ascending=False).iloc[0]['Cluster']\n",
    "risk_idx = summary.sort_values('Recency', ascending=False).iloc[0]['Cluster']\n",
    "\n",
    "def label_cluster(c):\n",
    "    if c == vip_idx: return 'High Value'\n",
    "    elif c == risk_idx: return 'At-Risk'\n",
    "    else: return 'Average'\n",
    "\n",
    "rfm_clean['Segment_Label'] = rfm_clean['Cluster'].apply(label_cluster)\n",
    "rfm_clean['Target_Label'] = rfm_clean['Segment_Label'].map({'At-Risk': 0, 'Average': 1, 'High Value': 2})\n",
    "\n",
    "# Calculate Future Spend (Target)\n",
    "future_spend = df_future.assign(Spend=df_future['Quantity'] * df_future['UnitPrice']) \\\n",
    "                        .groupby('CustomerID')['Spend'].sum() \\\n",
    "                        .rename('FutureSales_Label')\n",
    "\n",
    "rfm_final = rfm_clean.merge(future_spend, on='CustomerID', how='left').fillna(0)\n",
    "\n",
    "# EVALUATION & EXPORT\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLUSTERING METRICS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sil_score = silhouette_score(rfm_scaled, rfm_clean['Cluster'])\n",
    "db_score = davies_bouldin_score(rfm_scaled, rfm_clean['Cluster'])\n",
    "ch_score = calinski_harabasz_score(rfm_scaled, rfm_clean['Cluster'])\n",
    "\n",
    "print(f\"{'Metric':<25} | {'Value':<10}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Silhouette Coeff':<25} | {sil_score:.4f}\")\n",
    "print(f\"{'Davies-Bouldin Index':<25} | {db_score:.4f}\")\n",
    "print(f\"{'Calinski-Harabasz':<25} | {ch_score:.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SEGMENT PROFILES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "profile = rfm_final.groupby('Segment_Label')[['Recency', 'Frequency', 'Avg_Monthly_Spend', 'Tenure', 'FutureSales_Label']].mean()\n",
    "profile['Count'] = rfm_final['Segment_Label'].value_counts()\n",
    "profile['Share (%)'] = (profile['Count'] / len(rfm_final)) * 100\n",
    "profile = profile[['Count', 'Share (%)', 'Recency', 'Frequency', 'Avg_Monthly_Spend', 'FutureSales_Label']]\n",
    "\n",
    "print(profile.round(2).to_string())\n",
    "\n",
    "# Save artifacts\n",
    "if not os.path.exists(os.path.dirname(PROCESSED_PATH)): os.makedirs(os.path.dirname(PROCESSED_PATH))\n",
    "rfm_final.to_csv(PROCESSED_PATH)\n",
    "print(f\"\\nSaved processed data to: {PROCESSED_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed8ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
